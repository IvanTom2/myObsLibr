Пусть задан вектор входных переменных $X^{T}=(X_1,X_2,...,X_p)$. С помощью него мы предсказываем значение выходной переменной $Y$ через линейную модель:
$$\hat{Y}=\hat{\beta}_0 + \sum_{j=1}^p X_j \hat{\beta}_j$$
Слагаемое $\hat{\beta}_0$ - называется ***сдвигом*** (***intercept***), а также ***смещение*** (***bias***). 
Часто удобно вектор $X$ дополнить постоянным элементом, который равен 1. Это позволяет включить величину $\hat{\beta}_0$ в вектор коэффициентов $\hat{\beta}$ и записать линейную модель в векторной форме как скалярное произведение:
$$\hat{Y}=X^T \hat{\beta}$$

В текущем случае мы моделируем одну выходную переменную, поэтому $\hat{Y}$ - это скаляр. В общем случае $\hat{Y}$ может быть вектором, состоящем из $K$ элементов - в этом случае $\beta$ будет матрицей коэффициентов размером $p \times K$. 

Функция $f(X)=X^T \beta$, заданная на $p$-мерном пространстве входных данных, является линейной. Градиент $f^{'}(X)=\beta$ представляет собой вектор в пространстве выходных переменных. который указывает самое крутое направление подъема. 

***Метод наименьших квадратов***. При использовании данного подхода мы выбираем коэффициенты $\beta$ так, чтобы минимизировать остаточную сумму квадратов:
$$RSS(\beta)=\sum_{i=1}^{N}(y_i - x^T_i \beta)^2$$
В матричном виде МНК можно записать следующим образом:
$$RSS(\beta)=(y-X \beta)^T(y-X \beta)$$
Здесь $X$ - это матрица $N \times p$, в которой каждая строка является вектором входных переменных. $y$ - это вектор, содержащий $N$ значений выходной переменной из обучающего множества. 

Если провести дифференцирование, получим ***систему нормальных уравнений***: 
$$X^T(y-X \beta)=0$$
Если матрица $X^TX$ является ***невырожденной***, то ***единственное решение*** задается формулой: 
$$\hat{\beta}=(X^TX)^{-1}X^Ty$$

 При произвольном значении входной переменной $x_0$ предсказанное значение будет $\hat{y}(x_0)=x^T_0 \hat{\beta}$. Аппроксимированная поверхность характеризируется $p$ параметрами $\hat{\beta}$. 

![[Pasted image 20230804175811.png]]

___
Links: [[Основы статистического обучения]] 
Tags:
References: