Используется EM-алгоритм (expectation maximization). 

Каждый кластер описывается в терминах плотности нормального распределения, которая имеет центроид и ковариационную матрицу. Сравнение становится более четким, если потребовать, чтобы нормально распределенные компоненты имели скалярную матрицу ковариации. Два шага чередующегося EM-алгоритма похожи на два шага в алгоритме k-средних:
1. На E-шаге каждому наблюдению присваивается ***ответственность*** (вес) для каждого кластера на основе прадоподобия каждого из соответствующих нормальных распределений. Наблюдения, близкие к центру кластера, скорее всего получат единичный вес для этого кластера и нулевой для остальных. Наблюдения, которые находятся между скоплениями, делят их вес соответственно. 
2. На M-этапе каждое наблюдение вносит вклад во взвешенные средние (и ковариационную матрицу) для каждого кластера. 

Модели смеси нормальных распределений часто называют методом мягкой кластеризации. 

___
Зачем это нужно: [[]] 
Примеры применения: [[]] 
Основные ошибки: [[]]
___
Relations: [[Методы прототипов и ближайших соседей kNN]] 
Tags: #theory 
References: [[Методы прототипов и ближайших соседей kNN]] 
Query: 