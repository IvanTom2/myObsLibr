Term Frequency и Inverse Document Frequency (TF-IDF).

TF подсчитывает, сколько раз токен встречается ***в текущем документе*** и делит его на общее количество токенов этого же документа. Другими словами, количество текущих токенов (одинаковых) делится на общее количество токенов.
$$TF = \frac{N_{D}(q=token)}{N_{D}}$$

IDF подсчитывает, сколь раз термин (токен) встречается ***во всех документах***. Другими словами, подсчитывается логарифм того, в скольки из документах представлен текущий токен - т.е. количество повторений не учитывается, учитывается только есть ли хотя бы один токен в документе и делится на общее количество документов. 
$$IDF=\log{\frac{N}{N(q=token)}}=\log{\frac{|D|}{|\{d_{i}\in{D}|t\in{d_{i}}\}|}}$$

Для каждого токена вычисляется TF и IDF по всем документами. IDF для токена вычисляется только один раз, т.к. относится ко всем документами. Далее TF умножается на IDF. 
$$TF-IDF=TF*IDF$$

По итогу, для каждого документа создаются разряженные векторы. Они получаются путем добавления в массив TF-IDF значений по объединению множеств всех токенов. Т.е. TF-IDF для любого токена из множества всех токенов в текущем документе представлен в любом случае, даже не смотря на то, что в документе этого токена нет - отсюда и разряженный вектор. 

Для сравнения векторов используется кластерный анализ. Так, можно применить евклидово расстояние, косинусную меры, манхэттенское расстояние, расстояние Чебышёва и другие. 

Одна из проблем TF-IDF связана с рассчётом нескольких упоминаний. Например, если в документа А токен представлен 6 раз, а в документе Б 12 раз, то должны ли мы считать статью А в 2 раза менее релевантной? 
___
Links: [[NLP]] [[Векторное сходство]]
Tags: #nlp 
References: